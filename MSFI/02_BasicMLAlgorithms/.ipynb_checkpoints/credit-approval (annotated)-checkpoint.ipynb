{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification - Credit Approval\n",
    "\n",
    "Dataset:\n",
    "https://archive.ics.uci.edu/ml/datasets/Credit+Approval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset observations\n",
    "\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.names\n",
    "\n",
    "- Class distribution is quite balanced\n",
    "- Columns are anonymized\n",
    "- There are missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "Data Gathering\n",
    "1. read_csv\n",
    "\n",
    "Data Transformation\n",
    "2. transform dataframe\n",
    "3. PCA to plot (for classification)\n",
    "4. train-test split\n",
    "5. scale\n",
    "\n",
    "Training\n",
    "6. logistic regression\n",
    "7. SGD logistic regression\n",
    "\n",
    "Validation\n",
    "8. metrics\n",
    "9. learning curve\n",
    "10. prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering\n",
    "\n",
    "1. read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'D:/tmp/credit-approval/crx.data' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-92732e639b81>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m                       \u001b[1;34m'A7'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'A8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'A9'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'A10'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'A11'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'A12'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                       'A13', 'A14', 'A15', 'y'],\n\u001b[1;32m----> 5\u001b[1;33m                 na_values=['?', 'nan'])\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\mldds02\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\mldds02\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\mldds02\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\mldds02\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\mldds02\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'D:/tmp/credit-approval/crx.data' does not exist"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('D:/tmp/credit-approval/crx.data',\n",
    "                names=['A1', 'A2', 'A3', 'A4', 'A5', 'A6',\n",
    "                      'A7', 'A8', 'A9', 'A10', 'A11', 'A12',\n",
    "                      'A13', 'A14', 'A15', 'y'],\n",
    "                na_values=['?', 'nan'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation\n",
    "2. transform dataframe\n",
    "3. PCA to plot (for classification)\n",
    "4. train-test split\n",
    "5. scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. transform dataframe\n",
    "  - change to numeric types\n",
    "  - handle NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we cannot interpolate the values, we'll drop them\n",
    "# drop the NaN values before we perform encoding\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.A1.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.A4.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try one-hot encoding\n",
    "columns_to_encode = ['A1', 'A4', 'A5', 'A6', 'A7', 'A9', 'A10', 'A12', 'A13']\n",
    "np.testing.assert_array_equal(columns_to_encode, df.loc[:, columns_to_encode].columns)\n",
    "\n",
    "dummies = pd.get_dummies(df.loc[:, columns_to_encode])\n",
    "dummies.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.concat([df, dummies], axis=1)\n",
    "df_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's clean up some columns\n",
    "df_1.drop(columns_to_encode, axis=1, inplace=True)\n",
    "df_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now check to make sure dtypes are all numeric\n",
    "df_1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the last one we deal with is class\n",
    "# since this is the classification output, the convention is to use 1, 0\n",
    "df_1.y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use sklearn.preprocessing.LabelEncoder, but that doesn't give us the\n",
    "# ability to assign labels to numbers. For example, if we want '+' to be 1,\n",
    "# and '-' to be 0. This is because LabelEncoder picks the first class it encounters\n",
    "# and assigned the number accordingly.\n",
    "\n",
    "y_enc = df_1.y.map({'+': 1, '-': 0})\n",
    "y_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.drop(['y'], axis=1, inplace=True) # drop the original y column\n",
    "df_2 = pd.concat([df_1, y_enc], axis=1) # add the encoded y column\n",
    "df_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. PCA to plot (for classification)\n",
    "\n",
    "  - Plot a scatter plot with 2 feature dimensions (or 3 feature dimensions)  \n",
    "  - Use colours for y_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "\n",
    "X = df_2.loc[:, 'A2':'A13_s']\n",
    "y = df_2.y\n",
    "\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_3d = pca.fit_transform(X)\n",
    "print('Before:', X.shape, 'After:', X_3d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A better tutorial:\n",
    "# https://matplotlib.org/gallery/mplot3d/scatter3d.html\n",
    "\n",
    "# interactive plot\n",
    "%matplotlib notebook\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(X_3d[y==0, 0], X_3d[y==0, 1], X_3d[y==0, 2], color='r', label='y=0')\n",
    "ax.scatter(X_3d[y==1, 0], X_3d[y==1, 1], X_3d[y==1, 2], color='b', label='y=1')\n",
    "ax.set(xlabel='X_3d[:, 0]', ylabel='X_3d[:, 1]', zlabel='X_3d[:, 2]',\n",
    "       title='PCA plot of Credit Approval dataset')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot looks squished, let's try scaling the features first to see if we get a better view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are plotting all the datapoints, so we want to fit to the whole dataset\n",
    "# during training/testing, we'll still fit separately.\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_3d = pca.fit_transform(X_scaled)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(X_3d[y==0, 0], X_3d[y==0, 1], X_3d[y==0, 2], color='r', label='y=0')\n",
    "ax.scatter(X_3d[y==1, 0], X_3d[y==1, 1], X_3d[y==1, 2], color='b', label='y=1')\n",
    "ax.set(xlabel='X_3d[:, 0]', ylabel='X_3d[:, 1]', zlabel='X_3d[:, 2]',\n",
    "       title='Scaled PCA plot of Credit Approval dataset')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, lastly, let's try a 2D plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are plotting all the datapoints, so we want to fit to the whole dataset\n",
    "# during training/testing, we'll still fit separately.\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X_scaled)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.scatter(X_2d[y==0, 0], X_2d[y==0, 1], color='r', label='y=0')\n",
    "ax.scatter(X_2d[y==1, 0], X_2d[y==1, 1], color='b', label='y=1', alpha=.2) # alpha sets transparency\n",
    "ax.set(xlabel='X_2d[:, 0]', ylabel='X_2d[:, 1]',\n",
    "       title='Scaled PCA plot of Credit Approval dataset')\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, visualization is also iterative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll split the unscaled features\n",
    "# then scale them using just the mean & variance of the training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = StandardScaler()\n",
    "X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "# note that you don't scale y. It's a class output, which has only individual (discrete)\n",
    "# values such as 0 vs 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "6. logistic regression\n",
    "7. SGD logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(random_state=42)\n",
    "logistic.fit(X_train_scaled, y_train)\n",
    "y_pred_logistic = logistic.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier(tol=1e-4, max_iter=1000, verbose=True, random_state=42)\n",
    "sgd.fit(X_train_scaled, y_train)\n",
    "y_pred_sgd = sgd.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "8. metrics\n",
    "9. learning curve\n",
    "10. prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report. See classification.ipynb for details\n",
    "print(classification_report(y_test, y_pred_logistic))\n",
    "print(classification_report(y_test, y_pred_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix. See classification.ipynb for details\n",
    "cm_logistic = confusion_matrix(y_test, y_pred_logistic)\n",
    "cm_sgd = confusion_matrix(y_test, y_pred_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -y seaborn\n",
    "\n",
    "# matplotlib can plot confusion matrices, isn't as easy as seaborn\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\n",
    "ax = axes.flatten()\n",
    "\n",
    "# annotate cells with ticks\n",
    "sns.heatmap(cm_logistic, annot=True, ax=ax[0])\n",
    "sns.heatmap(cm_sgd, annot=True, ax=ax[1])\n",
    "\n",
    "ax[0].set(xlabel='Predicted labels', ylabel='True labels', title='Confusion Matrix (Logistic Regression)') \n",
    "ax[0].xaxis.set_ticklabels(['Denied', 'Approved'])\n",
    "ax[0].yaxis.set_ticklabels(['Denied', 'Approved'])\n",
    "\n",
    "ax[1].set(xlabel='Predicted labels', ylabel='True labels', title='Confusion Matrix (Logistic Regression using SGD)'); \n",
    "ax[1].xaxis.set_ticklabels(['Denied', 'Approved'])\n",
    "ax[1].yaxis.set_ticklabels(['Denied', 'Approved'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_2 = LogisticRegression(random_state=42)\n",
    "train_sizes, train_score, val_score = learning_curve(logistic_2, X_train_scaled, y_train)\n",
    "\n",
    "train_mean = np.mean(train_score, axis=1)\n",
    "val_mean = np.mean(val_score, axis=1)\n",
    "\n",
    "print('train_size', 'mean_train_score (3-fold cv)', 'mean_val_score (3-fold cv)')\n",
    "for train_size, t, m in zip(train_sizes, train_mean, val_mean):\n",
    "    print(train_size, t, m)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "ax.plot(train_sizes, train_mean, label='train score', marker='x')\n",
    "ax.plot(train_sizes, val_mean, label='val score', marker='o')\n",
    "\n",
    "# LogisticRegression.score() is the mean accuracy\n",
    "ax.set(xlabel='train size', ylabel='mean accuracy', title='Learning Curve for Logistic Regression')\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sgd_2 = SGDClassifier(tol=1e-4, max_iter=1000, random_state=42)\n",
    "train_sizes, train_score, val_score = learning_curve(sgd_2, X_train_scaled, y_train)\n",
    "\n",
    "train_mean = np.mean(train_score, axis=1)\n",
    "val_mean = np.mean(val_score, axis=1)\n",
    "\n",
    "print('train_size', 'mean_train_score (3-fold cv)', 'mean_val_score (3-fold cv)')\n",
    "for train_size, t, m in zip(train_sizes, train_mean, val_mean):\n",
    "    print(train_size, t, m)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "ax.plot(train_sizes, train_mean, label='train score', marker='x')\n",
    "ax.plot(train_sizes, val_mean, label='val score', marker='o')\n",
    "\n",
    "# LogisticRegression.score() is the mean accuracy\n",
    "ax.set(xlabel='train size', ylabel='mean accuracy', title='Learning Curve for Logistic Regression (SGD)')\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of 164 points:\n",
      "Logistic Regression: 53, Mean Accuracy: 0.677\n",
      "Logistic Regression (SGD): 52, Mean Accuracy: 0.683\n",
      "\n",
      "Truth (1=approved, 0=denied) [0 0 1 0 0 0 0 1 0 0]\n",
      "Logistic Regression [1 1 1 1 0 1 1 1 0 1]\n",
      "Logistic Regression with SGD [1 1 1 1 0 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "test = X_test\n",
    "truth = y_test.values\n",
    "\n",
    "pred_lr = logistic.predict(test)\n",
    "pred_sgd = sgd.predict(test)\n",
    "\n",
    "print('Number of mislabeled points out of %d points:' % test.shape[0])\n",
    "print('Logistic Regression: %d, Mean Accuracy: %.3f' % ((truth != pred_lr).sum(),\n",
    "                                              logistic.score(test, truth)))\n",
    "print('Logistic Regression (SGD): %d, Mean Accuracy: %.3f' % ((truth != pred_sgd).sum(),\n",
    "                                                            sgd.score(test, truth)))\n",
    "\n",
    "# print first 10 test datapoints and predictions\n",
    "print()\n",
    "print('Truth (1=approved, 0=denied)', truth[:10])\n",
    "print('Logistic Regression', pred_lr[:10])\n",
    "print('Logistic Regression with SGD', pred_sgd[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "This is a probability-based classifier using the Bayes Theorem.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "\n",
    "The probability distribution used here is Gaussian. There are other distributions supported (multinomial, bernoulli), depending on how the dataset could be distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9d0cc747d6fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0my_pred_nb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train_scaled, y_train)\n",
    "y_pred_nb = nb.predict(X_test_scaled)\n",
    "\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "cm_nb = confusion_matrix(y_test, y_pred_nb)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(cm_nb, annot=True, ax=ax)\n",
    "\n",
    "ax.set(xlabel='Predicted labels', ylabel='True labels', title='Confusion Matrix (Gaussian Naive Bayes)') \n",
    "ax.xaxis.set_ticklabels(['Denied', 'Approved'])\n",
    "ax.yaxis.set_ticklabels(['Denied', 'Approved'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning curve\n",
    "train_sizes, train_score, val_score = learning_curve(GaussianNB(), X_train_scaled, y_train)\n",
    "\n",
    "train_mean = np.mean(train_score, axis=1)\n",
    "val_mean = np.mean(val_score, axis=1)\n",
    "\n",
    "print('train_size', 'mean_train_score (3-fold cv)', 'mean_val_score (3-fold cv)')\n",
    "for train_size, t, m in zip(train_sizes, train_mean, val_mean):\n",
    "    print(train_size, t, m)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "ax.plot(train_sizes, train_mean, label='train score', marker='x')\n",
    "ax.plot(train_sizes, val_mean, label='val score', marker='o')\n",
    "\n",
    "ax.set(xlabel='train size', ylabel='mean accuracy', title='Learning Curve for Gaussian Naive Bayes')\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier\n",
    "\n",
    "We'll try the SVC classifier with the default kernel (radial basis function). \n",
    "\n",
    "This kernel function is used to generate the predictions for a test data value, based on finding the separation boundary (hyperplane) with the largest margin.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "Other supported kernels are: linear, polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "y_pred_svc = svc.predict(X_test_scaled)\n",
    "\n",
    "print(classification_report(y_test, y_pred_svc))\n",
    "cm_nb = confusion_matrix(y_test, y_pred_svc)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(cm_nb, annot=True, ax=ax)\n",
    "\n",
    "ax.set(xlabel='Predicted labels', ylabel='True labels', title='Confusion Matrix (SVM with RBF kernel)') \n",
    "ax.xaxis.set_ticklabels(['Denied', 'Approved'])\n",
    "ax.yaxis.set_ticklabels(['Denied', 'Approved'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning curve\n",
    "train_sizes, train_score, val_score = learning_curve(SVC(), X_train_scaled, y_train)\n",
    "\n",
    "train_mean = np.mean(train_score, axis=1)\n",
    "val_mean = np.mean(val_score, axis=1)\n",
    "\n",
    "print('train_size', 'mean_train_score (3-fold cv)', 'mean_val_score (3-fold cv)')\n",
    "for train_size, t, m in zip(train_sizes, train_mean, val_mean):\n",
    "    print(train_size, t, m)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "ax.plot(train_sizes, train_mean, label='train score', marker='x')\n",
    "ax.plot(train_sizes, val_mean, label='val score', marker='o')\n",
    "\n",
    "ax.set(xlabel='train size', ylabel='mean accuracy', title='Learning Curve for SVM with RBF kernel')\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-nearest Neighbours Classifier\n",
    "\n",
    "The last classifier we'll try is K-nearest neighbours.\n",
    "\n",
    "As the name implies, this looks at the k-closest neighbouring labeled datapoints, and determines (by majority vote) which class a test data point belongs to.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "kn = KNeighborsClassifier()\n",
    "kn.fit(X_train_scaled, y_train)\n",
    "y_pred_kn = kn.predict(X_test_scaled)\n",
    "\n",
    "print(classification_report(y_test, y_pred_kn))\n",
    "cm_nb = confusion_matrix(y_test, y_pred_kn)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(cm_nb, annot=True, ax=ax)\n",
    "\n",
    "ax.set(xlabel='Predicted labels', ylabel='True labels', title='Confusion Matrix (k=5 nearest neighbours)') \n",
    "ax.xaxis.set_ticklabels(['Denied', 'Approved'])\n",
    "ax.yaxis.set_ticklabels(['Denied', 'Approved'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning curve\n",
    "train_sizes, train_score, val_score = learning_curve(KNeighborsClassifier(), X_train_scaled, y_train)\n",
    "\n",
    "train_mean = np.mean(train_score, axis=1)\n",
    "val_mean = np.mean(val_score, axis=1)\n",
    "\n",
    "print('train_size', 'mean_train_score (3-fold cv)', 'mean_val_score (3-fold cv)')\n",
    "for train_size, t, m in zip(train_sizes, train_mean, val_mean):\n",
    "    print(train_size, t, m)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "ax.plot(train_sizes, train_mean, label='train score', marker='x')\n",
    "ax.plot(train_sizes, val_mean, label='val score', marker='o')\n",
    "\n",
    "ax.set(xlabel='train size', ylabel='mean accuracy', title='Learning Curve for k=5 Nearest Neighbors')\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Boundaries\n",
    "\n",
    "In classification (and clustering) problems, it is helpful to visualize how each model defines the boundaries separating the classes.\n",
    "\n",
    "This can also help give some intuition on what each classifier does, and more helpfully, how the boundaries can change with different hyperparameters.\n",
    "\n",
    "To do this, we fit each model on the 2-dimensional features and then plot a contour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper plotting functions to plot boundaries\n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    \"\"\"Create a mesh of points to plot in\n",
    "\n",
    "    Args:\n",
    "        x: data to base x-axis meshgrid on\n",
    "        y: data to base y-axis meshgrid on\n",
    "        h: stepsize for meshgrid, optional\n",
    "\n",
    "    Returns:\n",
    "        xx, yy : ndarray\n",
    "    \"\"\"\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Args:\n",
    "        ax: matplotlib axes object\n",
    "        clf: a classifier\n",
    "        xx: meshgrid ndarray\n",
    "        yy: meshgrid ndarray\n",
    "        params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For visualization purposes only\n",
    "pca = PCA(n_components=2)\n",
    "X_train_scaled_2d = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "X0, X1 = X_train_scaled_2d[:, 0], X_train_scaled_2d[:, 1]\n",
    "xx, yy = make_meshgrid(X0, X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "logistic_2d = LogisticRegression()\n",
    "logistic_2d.fit(X_train_scaled_2d, y_train)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plot_contours(ax, logistic_2d, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "ax.scatter(X0, X1, c=y_train, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "ax.set_xlim(xx.min(), xx.max())\n",
    "ax.set_ylim(yy.min(), yy.max())\n",
    "ax.set(xlabel='X_train_scaled_2d[:, 0]', ylabel='X_train_scaled_2d[:, 1]',\n",
    "       title='2d boundary plot (Logistic Regression)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "nb_2d = GaussianNB()\n",
    "nb_2d.fit(X_train_scaled_2d, y_train)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plot_contours(ax, nb_2d, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "ax.scatter(X0, X1, c=y_train, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "ax.set_xlim(xx.min(), xx.max())\n",
    "ax.set_ylim(yy.min(), yy.max())\n",
    "ax.set(xlabel='X_train_scaled_2d[:, 0]', ylabel='X_train_scaled_2d[:, 1]',\n",
    "       title='2d boundary plot (Naive Bayes with Gaussian distribution)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bernoulli Naive Bayes\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bnb_2d = BernoulliNB()\n",
    "bnb_2d.fit(X_train_scaled_2d, y_train)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plot_contours(ax, bnb_2d, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "ax.scatter(X0, X1, c=y_train, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "ax.set_xlim(xx.min(), xx.max())\n",
    "ax.set_ylim(yy.min(), yy.max())\n",
    "ax.set(xlabel='X_train_scaled_2d[:, 0]', ylabel='X_train_scaled_2d[:, 1]',\n",
    "       title='2d boundary plot (Naive Bayes with Bernoulli distribution)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with RBF kernel\n",
    "svc_2d = SVC()\n",
    "svc_2d.fit(X_train_scaled_2d, y_train)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plot_contours(ax, svc_2d, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "ax.scatter(X0, X1, c=y_train, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "ax.set_xlim(xx.min(), xx.max())\n",
    "ax.set_ylim(yy.min(), yy.max())\n",
    "ax.set(xlabel='X_train_scaled_2d[:, 0]', ylabel='X_train_scaled_2d[:, 1]',\n",
    "       title='2d boundary plot (SVM with RBF kernel)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with polynomial kernel\n",
    "svc_2d_poly = SVC(kernel='poly') # defaults to degrees=3\n",
    "svc_2d_poly.fit(X_train_scaled_2d, y_train)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plot_contours(ax, svc_2d_poly, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "ax.scatter(X0, X1, c=y_train, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "ax.set_xlim(xx.min(), xx.max())\n",
    "ax.set_ylim(yy.min(), yy.max())\n",
    "ax.set(xlabel='X_train_scaled_2d[:, 0]', ylabel='X_train_scaled_2d[:, 1]',\n",
    "       title='2d boundary plot (SVM with 3-degree polynomial kernel)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-nearest neighbours, k=5 (default)\n",
    "kn_2d = KNeighborsClassifier()\n",
    "kn_2d.fit(X_train_scaled_2d, y_train)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plot_contours(ax, kn_2d, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "ax.scatter(X0, X1, c=y_train, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "ax.set_xlim(xx.min(), xx.max())\n",
    "ax.set_ylim(yy.min(), yy.max())\n",
    "ax.set(xlabel='X_train_scaled_2d[:, 0]', ylabel='X_train_scaled_2d[:, 1]',\n",
    "       title='2d boundary plot (k=5 Nearest Neighbours)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-nearest neighbours, k=3\n",
    "kn_2d = KNeighborsClassifier(n_neighbors=3)\n",
    "kn_2d.fit(X_train_scaled_2d, y_train)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plot_contours(ax, kn_2d, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "ax.scatter(X0, X1, c=y_train, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "ax.set_xlim(xx.min(), xx.max())\n",
    "ax.set_ylim(yy.min(), yy.max())\n",
    "ax.set(xlabel='X_2d[:, 0]', ylabel='X_2d[:, 1]',\n",
    "       title='2d boundary plot (k=3 Nearest Neighbours)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-nearest neighbours, k=10\n",
    "kn_2d = KNeighborsClassifier(n_neighbors=10)\n",
    "kn_2d.fit(X_train_scaled_2d, y_train)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plot_contours(ax, kn_2d, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "ax.scatter(X0, X1, c=y_train, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "ax.set_xlim(xx.min(), xx.max())\n",
    "ax.set_ylim(yy.min(), yy.max())\n",
    "ax.set(xlabel='X_2d[:, 0]', ylabel='X_2d[:, 1]',\n",
    "       title='2d boundary plot (k=10 Nearest Neighbours)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Quality\n",
    "\n",
    "Area under ROC curve - the larger the better.\n",
    "\n",
    "![example](https://upload.wikimedia.org/wikipedia/commons/thumb/3/36/ROC_space-2.png/1024px-ROC_space-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparison, let's see how an ROC curve for a dummy classifier looks like\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Baseline\n",
    "baseline = DummyClassifier(random_state=42)\n",
    "baseline.fit(X_train_scaled, y_train)\n",
    "y_confidence_baseline = baseline.predict_proba(X_test_scaled)\n",
    "\n",
    "# predict_proba: prediction confidence\n",
    "# decision_function: distance to the decision boundary/hyperplane\n",
    "\n",
    "# y_confidence_baseline[:, 0] returns probabilities for class 0\n",
    "# y_confidence_baseline[:, 1] returns probabilities for class 1\n",
    "fpr_baseline, tpr_baseline, _ = roc_curve(y_test, y_confidence_baseline[:, 1], pos_label=1)\n",
    "auc_baseline = auc(fpr_baseline, tpr_baseline)\n",
    "\n",
    "# Logistic Regression\n",
    "y_confidence_logistic = logistic.predict_proba(X_test_scaled)\n",
    "fpr_logistic, tpr_logistic, _ = roc_curve(y_test, y_confidence_logistic[:, 1], pos_label=1)\n",
    "auc_logistic = auc(fpr_logistic, tpr_logistic)\n",
    "\n",
    "# Naive Bayes\n",
    "y_confidence_nb = nb.predict_proba(X_test_scaled)\n",
    "fpr_nb, tpr_nb, _ = roc_curve(y_test, y_confidence_nb[:, 1], pos_label=1)\n",
    "auc_nb = auc(fpr_nb, tpr_nb)\n",
    "\n",
    "# Support Vector Machine\n",
    "y_confidence_svc = svc.decision_function(X_test_scaled)\n",
    "fpr_svc, tpr_svc, _ = roc_curve(y_test, y_confidence_svc, pos_label=1)\n",
    "auc_svc = auc(fpr_svc, tpr_svc)\n",
    "\n",
    "# K-nearest Neighbor\n",
    "y_confidence_kn = kn.predict_proba(X_test_scaled)\n",
    "fpr_kn, tpr_kn, _ = roc_curve(y_test, y_confidence_kn[:, 1], pos_label=1)\n",
    "auc_kn = auc(fpr_kn, tpr_kn)\n",
    "\n",
    "# Plot the ROCs\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax.plot(fpr_baseline, tpr_baseline, label='Baseline (area = %0.2f)' % auc_baseline,\n",
    "        linestyle='dashed')\n",
    "ax.plot(fpr_logistic, tpr_logistic, label='Logistic Regression (area = %0.2f)' % auc_logistic)\n",
    "ax.plot(fpr_nb, tpr_nb, label='Naive Bayes (area = %0.2f)' % auc_nb)\n",
    "ax.plot(fpr_svc, tpr_svc, label='SVM (area = %0.2f)' % auc_svc)\n",
    "ax.plot(fpr_kn, tpr_kn, label='K Nearest Neighbors (area = %0.2f)' % auc_kn)\n",
    "\n",
    "# bigger area is better\n",
    "ax.set(xlabel='false positive rate', ylabel='true positive rate',\n",
    "       title='ROC curves for Credit Approval Dataset (positive label=1)')\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve - how is it derived\n",
    "\n",
    "- y-axis = True positive rate\n",
    "- x-axis = False positive rate\n",
    "\n",
    "What it does:\n",
    "1. Gets a subset of thresholds from the confidences or decision_function output (how far from decision boundary) from predicting X_test_scaled\n",
    "2. Sorts the thresholds, dropping duplicates.\n",
    "3. At each threshold, preform a cumulative sum to get its true positive count. Derive the false positive count from the true positive count.\n",
    "4. Compute the tpr and fpr from the counts.\n",
    "\n",
    "Source:\n",
    "- https://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/metrics/ranking.py#L453"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fpr_logistic, tpr_logistic, threshold = roc_curve(y_test, y_confidence_logistic[:, 1],\n",
    "                                                  pos_label=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax.plot(fpr_logistic, tpr_logistic, label='Logistic Regression (area = %0.2f)' % auc_logistic,\n",
    "       marker='o')\n",
    "\n",
    "for i, (th, fpr, tpr) in enumerate(zip(threshold, fpr_logistic, tpr_logistic)):\n",
    "    # label every few points\n",
    "    if i % 5 == 0:\n",
    "        ax.annotate('%.2f' % th, (fpr, tpr))\n",
    "\n",
    "ax.set(xlabel='false positive rate', ylabel='true positive rate',\n",
    "       title='ROC curve for Credit Approval Dataset (positive label=1)')\n",
    "\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(threshold[:5]) # prediction confidence values\n",
    "print(fpr_logistic[:5]) # false positive rates\n",
    "print(tpr_logistic[:5]) # true positive rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction confidence for each X_test_scaled\n",
    "# either the estimator.predict_proba() or the estimator.decision_function()\n",
    "#\n",
    "# only a subset will be selected to be thresholds\n",
    "# (drop colinear (same line) and duplicates)\n",
    "#\n",
    "print(y_confidence_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
